{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/contactmansi/Multimodal-Fake-News-Detection-Reddit/blob/master/CNN_2_way_text_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "479fe902"
      },
      "source": [
        "# CNN\n",
        "\n",
        "We start by importing the libraries."
      ],
      "id": "479fe902"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33bc9936",
        "outputId": "e210258b-bf4c-4b36-bf4e-cc44d939c69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import re\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2 \n",
        "import multiprocessing as mp\n",
        "# import FunctionsTFM \n",
        "import imp\n",
        "import threading\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "33bc9936"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeGlZfzKab2Y",
        "outputId": "2c981dd0-4a61-45bb-c65e-f569e90e8ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "jeGlZfzKab2Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5df1b35a"
      },
      "source": [
        "We also import the data."
      ],
      "id": "5df1b35a"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a27cd18f"
      },
      "outputs": [],
      "source": [
        "# Train data \n",
        "traindata_all = pd.read_csv('/content/drive/MyDrive/CS5344/Dataset/Fakeddit datasetv2.0/all_samples/all_train.tsv', sep='\\t')\n",
        "# Validation data \n",
        "validata_all = pd.read_csv('/content/drive/MyDrive/CS5344/Dataset/Fakeddit datasetv2.0/all_samples/all_validate.tsv', sep='\\t')\n",
        "# Test data \n",
        "testdata_all = pd.read_csv('/content/drive/MyDrive/CS5344/Dataset/Fakeddit datasetv2.0/all_samples/all_test_public.tsv', sep='\\t')"
      ],
      "id": "a27cd18f"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLbGRfpNeln9",
        "outputId": "8718f492-f51a-4a7f-d874-b2b1648d3849"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "878218"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "len(traindata_all)"
      ],
      "id": "YLbGRfpNeln9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56eed200"
      },
      "source": [
        "We select a subset of the dataframe with no missing values in the 'clean_title' column."
      ],
      "id": "56eed200"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-A_VB-od1cN",
        "outputId": "a71375fc-54be-4eb8-8b4c-6bfaa024c841"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(802789, 84536, 84481)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# drop null\n",
        "\n",
        "# train_2_way = traindata_all.dropna(subset=['clean_title', '2_way_label'])\n",
        "# valid_2_way = validata_all.dropna(subset=['clean_title', '2_way_label'])\n",
        "# test_2_way = testdata_all.dropna(subset=['clean_title', '2_way_label'])\n",
        "\n",
        "# train_3_way = traindata_all.dropna(subset=['clean_title','3_way_label'])\n",
        "# train_6_way = traindata_all.dropna(subset=['clean_title', '6_way_label'])\n",
        "# len(train_2_way), len(valid_2_way), len(test_2_way), "
      ],
      "id": "3-A_VB-od1cN"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3574cfc",
        "outputId": "8e22aaf4-d38c-4df2-b539-6dcb89063629"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(802789, 84536, 84481)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Train data with no missing values\n",
        "# train_data = traindata_all[traindata_all['clean_title'].notnull().to_numpy()]\n",
        "train_data = traindata_all.dropna(subset=['clean_title', '2_way_label'])\n",
        "# Validation data with no missing values\n",
        "# valid_data = validata_all[validata_all['clean_title'].notnull().to_numpy()]\n",
        "valid_data = validata_all.dropna(subset=['clean_title', '2_way_label'])\n",
        "# Test data with no missing values\n",
        "# test_data = testdata_all[testdata_all['clean_title'].notnull().to_numpy()]\n",
        "test_data = testdata_all.dropna(subset=['clean_title', '2_way_label'])\n",
        "len(train_data), len(valid_data), len(test_data)"
      ],
      "id": "e3574cfc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d62f80a"
      },
      "source": [
        "And we separate the datasets into the texts and the labels"
      ],
      "id": "5d62f80a"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9149859d"
      },
      "outputs": [],
      "source": [
        "## Train data\n",
        "train_news = list(train_data['clean_title'])\n",
        "train_labels = list(train_data['2_way_label'])\n",
        "## Valid data\n",
        "valid_news = list(valid_data['clean_title'])\n",
        "valid_labels = list(valid_data['2_way_label'])\n",
        "## Test data\n",
        "test_news = list(test_data['clean_title'])\n",
        "test_labels = list(test_data['2_way_label'])"
      ],
      "id": "9149859d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a444c553"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "We define a function to preprocess the data. We remove punctuations and numbers and also multiple spaces."
      ],
      "id": "a444c553"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2571f879"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "id": "2571f879"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ab17f801"
      },
      "outputs": [],
      "source": [
        "# Remove puntuations and numbers and multiple spaces\n",
        "\n",
        "train_news_clean_1 = []\n",
        "valid_news_clean_1 = []\n",
        "test_news_clean_1 = []\n",
        "# Train\n",
        "for new in train_news:\n",
        "    train_news_clean_1.append(preprocess_text(new))\n",
        "# Validation\n",
        "for new in valid_news:\n",
        "    valid_news_clean_1.append(preprocess_text(new))\n",
        "# Test\n",
        "for new in test_news:\n",
        "    test_news_clean_1.append(preprocess_text(new))"
      ],
      "id": "ab17f801"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f1903e8"
      },
      "source": [
        "Now  we remove stop words and perform lemmatization. We define the function to do that."
      ],
      "id": "3f1903e8"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "79086ac8"
      },
      "outputs": [],
      "source": [
        "# Initialize  lemmatizer and  stop_words\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Function to remove stopwords and perform lemmatization\n",
        "def remove_stopwords_lem(text):\n",
        "    text = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "    # Lematization\n",
        "    lemmatized_text = []\n",
        "    for word in text:\n",
        "        word1 = lemmatizer.lemmatize(word, pos = \"n\")\n",
        "        word2 = lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "        word3 = lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "        lemmatized_text.append(word3)\n",
        "        \n",
        "    text_done = ' '.join(lemmatized_text)\n",
        "    return text_done"
      ],
      "id": "79086ac8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16ef9e76"
      },
      "source": [
        "And now perform stop-words removal and lemmatization."
      ],
      "id": "16ef9e76"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e81981a6"
      },
      "outputs": [],
      "source": [
        "# Stop-words removal and lemmatization\n",
        "train_stwrd_lem = []\n",
        "valid_stwrd_lem = []\n",
        "test_stwrd_lem = []\n",
        "\n",
        "# Train\n",
        "for new in train_news_clean_1:\n",
        "    train_stwrd_lem.append(remove_stopwords_lem(new))\n",
        "# Validation\n",
        "for new in valid_news_clean_1:\n",
        "    valid_stwrd_lem.append(remove_stopwords_lem(new))\n",
        "# Test\n",
        "for new in test_news_clean_1:\n",
        "    test_stwrd_lem.append(remove_stopwords_lem(new))"
      ],
      "id": "e81981a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eca516c"
      },
      "source": [
        "We train a tokenizer using all the documents and we use the learned vocabulary in order to transform texts into sequences of ID's."
      ],
      "id": "5eca516c"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ed842a",
        "outputId": "1baef4ae-fe92-4672-da89-4419d67c53da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971806\n"
          ]
        }
      ],
      "source": [
        "news_all = train_stwrd_lem + valid_stwrd_lem + test_stwrd_lem\n",
        "print(len(news_all))\n",
        "tokenizer = Tokenizer(num_words = 20000) # Why this number? -- look up paper\n",
        "tokenizer.fit_on_texts(news_all)\n",
        "\n",
        "# Tokenize news\n",
        "\n",
        "# Train\n",
        "train_tokenized = tokenizer.texts_to_sequences(train_stwrd_lem)\n",
        "# Validation\n",
        "valid_tokenized = tokenizer.texts_to_sequences(valid_stwrd_lem)\n",
        "# Test\n",
        "test_tokenized = tokenizer.texts_to_sequences(test_stwrd_lem)"
      ],
      "id": "21ed842a"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvrQKayPoiZf",
        "outputId": "3096bbc6-786d-48fe-e6aa-ebf62eea3bb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[7381, 15337, 6169, 749, 347, 255],\n",
              "  [1304, 624, 242, 523],\n",
              "  [2647, 912, 1375, 6760, 3115, 17],\n",
              "  [527, 1324],\n",
              "  [508, 15, 445],\n",
              "  [5, 25, 2379, 585],\n",
              "  [5160, 246, 37],\n",
              "  [1927, 2778, 2635, 2272, 2689, 56, 455],\n",
              "  [1252, 13569],\n",
              "  [5848, 1127, 3627, 771, 113, 9083, 1191, 68, 776]],\n",
              " ['walgreens offbrand mucinex engrave letter mucinex different order',\n",
              "  'concern sink tiny hat',\n",
              "  'hacker leak email uae ambassador u',\n",
              "  'flower neighborhood',\n",
              "  'puppy take view',\n",
              "  'find face sheet music',\n",
              "  'escobar couple think',\n",
              "  'bride groom exchange vow fatal shoot wed',\n",
              "  'major thermos',\n",
              "  'rabbi meat clone pig could kosher jew eat milk'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_tokenized[:10], train_stwrd_lem[:10]"
      ],
      "id": "GvrQKayPoiZf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS84b4HnhlEo"
      },
      "source": [
        "Obtain the vocabulary length"
      ],
      "id": "cS84b4HnhlEo"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPFdr3kQhp_B",
        "outputId": "897c1d04-998e-4082-ee60-e94daac78909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary length:  142720\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index)\n",
        "print(\"Vocabulary length: \", vocab_size)"
      ],
      "id": "UPFdr3kQhp_B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259c25be"
      },
      "source": [
        "Now we pad the sequences of numbers generated by the tokenizer. But firstly we check how many sequences are shorter than a given length. We start by defining a function that counts the length of each sequence."
      ],
      "id": "259c25be"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "a4f507be"
      },
      "outputs": [],
      "source": [
        "# Function to count the length of each sequence\n",
        "def length_squences(data):\n",
        "    lengths = []\n",
        "    for i in range(len(data)):\n",
        "        lengths.append(len(data[i]))\n",
        "    return lengths"
      ],
      "id": "a4f507be"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1df362bd"
      },
      "source": [
        "Now we count the lenghts of all the sequences in the training, validation and test set and we check what % of sequences in the train, validation and test sets are smaller than a given length when they are tokenized."
      ],
      "id": "1df362bd"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k_WvwCqkW5K",
        "outputId": "b99fed41-3fee-4c36-9f9a-a78ea60e42b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentages (train): [90.67986731258151, 98.47681022036923, 99.52640108422014, 99.85089481794095]\n",
            "Percentages (validation): [90.60755181224567, 98.44326677391881, 99.52919466262894, 99.83793886628182]\n",
            "Percentages (test): [90.52923142481741, 98.47302943857198, 99.50876528450185, 99.83901705708976]\n"
          ]
        }
      ],
      "source": [
        "length = [10, 15, 20, 25]\n",
        "\n",
        "# Train set\n",
        "lengths_train = np.array(length_squences(train_tokenized))\n",
        "perc_length_train = []\n",
        "for lgth in length:\n",
        "   perc_length_train.append( sum(lengths_train < lgth)/len(lengths_train)*100)\n",
        "print(\"Percentages (train):\", perc_length_train)\n",
        "\n",
        "# Validation set \n",
        "lengths_valid = np.array(length_squences(valid_tokenized))\n",
        "perc_length_valid = []\n",
        "for lgth in length:\n",
        "   perc_length_valid.append( sum(lengths_valid < lgth)/len(lengths_valid)*100)\n",
        "print(\"Percentages (validation):\", perc_length_valid)\n",
        "\n",
        "# Test set\n",
        "lengths_test = np.array(length_squences(test_tokenized))\n",
        "perc_length_test = []\n",
        "for lgth in length:\n",
        "   perc_length_test.append( sum(lengths_test < lgth)/len(lengths_test)*100)\n",
        "print(\"Percentages (test):\", perc_length_test)"
      ],
      "id": "8k_WvwCqkW5K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgG1riOjlxNb"
      },
      "source": [
        "We plot the results."
      ],
      "id": "JgG1riOjlxNb"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "UoF5Nrlll08D",
        "outputId": "d8b566a6-2f33-4127-bd07-a329e7212d2e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgV5Zn+8e9tI6CAItBBhCBo1Igii61EjQvBXHGLGqMG1ASC82MgiSaTKG6JMEZmdHQS42QSY8Y1MaLBuOCSqIy4jFEEgyyCERVDIyASWQwuoM/vj6ouj013c7r7LE33/bmuc/Wpeqveeuo91ec59damiMDMzAxgu3IHYGZmLYeTgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWcZJoQ2RdLmktyStLHcs5SbpZkmXp++PklRd5OUtlXR0MZdRaJIOl/RSGZZblraS1E9SSGpX6mW3JE4KLYykayS9LenPkvrkjD9D0rXNqLcv8ANgQETsWkd5wb4YS/El25LlJpxtWUQ8GRH7lDuOYtkWE3UpOCm0IJIOBg4EdgWeAi5Mx+8MnA/8sBnV9wXWRMSbzY3TPtbWf1Va6+Ok0LL0B56KiPeBGcAe6fgpwFURsb6hmSXtLOlWSaslvS7ph5K2S38NPQLsJukdSTfXmq8T8FBO+TuSdkvnvVDSK5LWSLpTUrd0nl9KuiunjislzWigroMlzZa0XtIqST+pZx16SLpf0lpJf5f0pKTt0rKlks6XNE/SPyTdIKmnpIckbZD0qKRdcur6vaSVktZJekLSfvl8CGm8d6Xt+Jqkc3PKJkuaJum3ktYDY2rNOw44E5iYrvv0nOLBaezrJN0hqWM6zy7pOq9O9xLvr7WXOFPSjyX9X7qeD0vqUU/sDdZVx/RDJf0lrff3aVxbdKtJukDStFrz/qxm7zXd9m6QtELSciVdlRVp2RhJT0m6Oo3pNUnH5vFRsJVtsKa7Z7SkvynpGr0kZ94dJN2SLnORpIk56/Mbkh9K09PPaWLOYs+sq742IyL8aiEvYH+SPYQdgKvSVxXwSJ7z3wrcC3QB+gF/Bc5Oy44CqhuYd4ty4LvAM0AfoAPwK+D2tGzHtP4xwOHAW0CfBur6M/D19H1n4HP1xPHvwHXA9unrcEBp2dI0np5Ab+BN4HlgCNAR+F9gUk5dY9O26ABcA8zNKbsZuLx2vCQ/lOYAlwLtSRLzq8CX0vLJwCbg5HTaHepYh6zunHFLgVnAbkA3YBEwPi3rDnw1bdMuwO+Be3LmnQm8AuydbhszgSvqab8G66o1bXvg9fRz3h44BfignnbZHdgIdEmHK4AVNZ8jcHe6fXQCPpWu6z+nZWPSNvt/6XwTgDdqPtc64loKHJ3HNtgPCODXabsMAt4H9k3LrwAeB3ZJ559HznaZu5x86msrr7IH4FetDwT+BXgBuAOoBJ4G9gXOBZ4AbgO61jFfRfoPPSBn3D8DM9P32T94Pcvdojz94hqRM9wr/edulw4PA/6efrGM2kpdTwD/CvTYyvpfRpLYPlNH2VLgzJzhu4Bf5gyfQ/1fgF3Tf/id0+GbqfvLbxjwt1rzXgTclL6fDDyxlXXI6q4V+1k5w/8BXFfP/IOBt3OGZwI/zBn+FvDHPLenT9RVq+wIYDk5X84kP0q2aJecsm+k778IvJK+75l+ee6QM+0o4LH0/RhgSU7ZjulnsWs9cS3l46RQ7zbIx1/ifXLKZwEj0/dZMk+H/4n8kkKd9bWVl7uPWpiI+GlEDIqIrwGnk3yZbgeMA0aQ/JNcWMesPUh+7b2eM+51kl/UTbU7cHfalbM2XfaHJF8CRMSzJP94Au7cSl1nk/zSXSzpOUkn1DPdVcAS4GFJr0qqva6rct6/W8dwZwBJFZKuSLsd1pN8AUDSTg3ZnaTra23Oel9Mus6pZVupoz65Z31tzIl1R0m/UtLlt57kM+9a0/3S0Ly15VlXjd2A5ZF++6UaWrffkXzZA5yRDkPSZtsDK3La7FckewxbxB8RG9O3da5DLQ1ug7Xr5pNts1ut9cn3c8urrVsrJ4UWSlJPkkRwGUm30ryI2AQ8BxxQxyxvkfyC2j1nXF+SX4L5qOt2ucuAYyOia86rY0QsT2P8Nsku/RvAxIbqioiXI2IUyRfFlcA0Jccfak+3ISJ+EBF7ACcC35c0Is91yHUGcBJwNLAzya9ASBJYQ5YBr9Va5y4RcVxumFupo7G3Hv4BsA8wLCJ2IvkFn0+sza1rBdBbUm7Zpxuo+/fAUekxiq/wcVJYRrKn0COnzXaKiLyO4WxFg9vgVqwg6TaqUXvdfIvoOjgptFw/ASanv6peAw6S1Jlkl/7V2hNHxIckv9anSOoiaXfg+8Bv81zeKqC7kjOdalyX1rc7gKRKSSel7/cGLgfOAr5OcmB1cH11STpLUmVEfASsTUd/VDsISSdI+kz6RbWO5FfhFtPloQvJF9Uaku6Kf8tzvlnAhvTA6g7pHsf+kg5qxLJX8fFJAvnG+i6wNj2IOqkR8zanrj+TtO93JLVLP9uD65s4IlaTdGXdRJI4F6XjVwAPA/8paaf04PCeko5sxnrUqHcbzMOdwEXpwffewHdqlTf2c2oTnBRaIElfIDlucDdARMwCHiD51TSc5ABaXc4B/kGSNJ4i+SV3Yz7LjIjFwO3Aq+mu+m7Az4D7SLpyNpAc8Bum5DTM3wJXRsQLEfEySRfLbyR1qKeuY4CFkt5J6x0ZEe/WEcpewKPAOyRfWr+IiMfyWYdabiXpPlsOvJjGnk87fAicQNIX/xrJHtj/kOxt5OsGYEC67vfkMf01JAc230rj/GMjltXkuiLiA5KDy2eTJOqzgPtJkml9fkey9/W7WuO/QXLg+kXgbWAaSf9/c9W5DeY572VANcnn+GgaU+66/Tvww/RzOq8AsbYKNWd1mJkh6VmSA+A3lTuWQpM0geTHSCH2YFot7ymYtWGSjpS0a9p9NJrkeFVz9lRaDEm9JB2WdmftQ3K85e5yx9XS+WpMs7ZtH5K+904k3Y6npscIWoP2JGdB9SfpHpsK/KKsEW0D3H1kZmYZdx+ZmVlmm+4+6tGjR/Tr16/cYZiZbVPmzJnzVkRU1lW2TSeFfv36MXv27HKHYWa2TZH0en1l7j4yM7OMk4KZmWWcFMzMLLNNH1Ooy6ZNm6iurua9994rdyitRseOHenTpw/bb799uUMxsyJrdUmhurqaLl260K9fPz5580driohgzZo1VFdX079//3KHY2ZF1uq6j9577z26d+/uhFAgkujevbv3vMzaiFaXFAAnhAJze5q1Ha0yKZiZWdO0umMKte169a6s+seqrU+Yp56derLyvJX1lq9Zs4YRI5IHha1cuZKKigoqK5MLB2fNmkX79u3rnXf27NnceuutXHvttQWL18ysMVp9UihkQsinvu7duzN37lwAJk+eTOfOnTnvvI+f37F582batau72auqqqiqqipcsGZmjeTuoxIYM2YM48ePZ9iwYUycOJFZs2ZxyCGHMGTIEA499FBeeuklAGbOnMkJJyTPs588eTJjx47lqKOOYo899vDeg5mVRKvfU2gpqqurefrpp6moqGD9+vU8+eSTtGvXjkcffZSLL76Yu+66a4t5Fi9ezGOPPcaGDRvYZ599mDBhgq8VMLOiKlpSkHQjybNu34yI/dNxVwFfBj4AXgG+GRFr07KLSJ4V+yFwbkT8qVixlcNpp51GRUUFAOvWrWP06NG8/PLLSGLTpk11znP88cfToUMHOnTowKc+9SlWrVpFnz59Shm2mbUxxew+upnkYe25HgH2j4gDgL8CFwFIGgCMBPZL5/mFpIoixlZynTp1yt7/6Ec/Yvjw4SxYsIDp06fXew1Ahw4dsvcVFRVs3ry56HGaWdtWtKQQEU8Af6817uGIqPlmewao+dl7EjA1It6PiNeAJcDBxYqt3NatW0fv3r0BuPnmm8sbjJlZjnIeaB4LPJS+7w0syymrTsdtQdI4SbMlzV69evVWF9KzU8/mxlnw+iZOnMhFF13EkCFD/OvfzFqUoj6jWVI/4P6aYwo54y8BqoBTIiIk/Rx4JiJ+m5bfADwUEdMaqr+qqipqP2Rn0aJF7LvvvoVbCQPcrmatiaQ5EVHn+e8lP/tI0hiSA9Aj4uOMtBz4dM5kfdJxZmZWQiXtPpJ0DDARODEiNuYU3QeMlNRBUn9gL2BWKWMzM7PinpJ6O3AU0ENSNTCJ5GyjDsAj6U3WnomI8RGxUNKdwIvAZuDbEfFhsWIzM7O6FS0pRMSoOkbf0MD0U4ApxYrHzMy2zre5MDOzjJOCmZllWn1S2HVXkAr32nXXhpc3fPhw/vSnT96h45prrmHChAl1Tn/UUUdRc1rtcccdx9q1a7eYZvLkyVx99dUNLveee+7hxRdfzIYvvfRSHn300YaDNTOrpdUnhVWFvXP2VusbNWoUU6dO/cS4qVOnMmpUXYdYPunBBx+ka9euTYqrdlK47LLLOProo5tUl5m1Xa0+KZTaqaeeygMPPMAHH3wAwNKlS3njjTe4/fbbqaqqYr/99mPSpEl1ztuvXz/eeustAKZMmcLee+/N5z//+ezW2gC//vWvOeiggxg0aBBf/epX2bhxI08//TT33Xcf559/PoMHD+aVV15hzJgxTJuWXPs3Y8YMhgwZwsCBAxk7dizvv/9+trxJkyYxdOhQBg4cyOLFi4vZNGa2DXBSKLBu3bpx8MEH89BDyR08pk6dyumnn86UKVOYPXs28+bN4/HHH2fevHn11jFnzhymTp3K3LlzefDBB3nuueeyslNOOYXnnnuOF154gX333ZcbbriBQw89lBNPPJGrrrqKuXPnsueee2bTv/fee4wZM4Y77riD+fPns3nzZn75y19m5T169OD5559nwoQJW+2iMrPWz0mhCHK7kGq6ju68806GDh3KkCFDWLhw4Se6emp78skn+cpXvsKOO+7ITjvtxIknnpiVLViwgMMPP5yBAwdy2223sXDhwgZjeemll+jfvz977703AKNHj+aJJ57Iyk855RQADjzwQJYuXdrUVTazVsJJoQhOOukkZsyYwfPPP8/GjRvp1q0bV199NTNmzGDevHkcf/zx9d4ue2vGjBnDz3/+c+bPn8+kSZOaXE+Nmttz+9bcZgZOCkXRuXNnhg8fztixYxk1ahTr16+nU6dO7LzzzqxatSrrWqrPEUccwT333MO7777Lhg0bmD59ela2YcMGevXqxaZNm7jtttuy8V26dGHDhg1b1LXPPvuwdOlSlixZAsBvfvMbjjzyyAKtqZm1Nq0+KfQs7J2z865v1KhRvPDCC4waNYpBgwYxZMgQPvvZz3LGGWdw2GGHNTjv0KFD+drXvsagQYM49thjOeigg7KyH//4xwwbNozDDjuMz372s9n4kSNHctVVVzFkyBBeeeWVbHzHjh256aabOO200xg4cCDbbbcd48ePb9xKm1mbUdRbZxebb51dOm5Xs9ajoVtnt/o9BTMzy5+TgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWaZoT15rMXbdtbC3Su3ZE1aurLd4zZo1jBgxAoCVK1dSUVFBZWUlALNmzaJ9+/YNVj9z5kzat2/PoYceWriYzczy1PqTQonvnd29e3fmzp0LJM9B6Ny5M+edd17e1c+cOZPOnTs7KZhZWbj7qATmzJnDkUceyYEHHsiXvvQlVqxYAcC1117LgAEDOOCAAxg5ciRLly7luuuu46c//SmDBw/mySefLHPkZtbWtP49hTKLCM455xzuvfdeKisrueOOO7jkkku48cYbueKKK3jttdfo0KEDa9eupWvXrowfP77RexdmZoXipFBk77//PgsWLOCLX/wiAB9++CG9evUC4IADDuDMM8/k5JNP5uSTTy5nmGZmgJNC0UUE++23H3/+85+3KHvggQd44oknmD59OlOmTGH+/PlliNDM7GM+plBkHTp0YPXq1VlS2LRpEwsXLuSjjz5i2bJlDB8+nCuvvJJ169bxzjvv1HsLbDOzUihaUpB0o6Q3JS3IGddN0iOSXk7/7pKOl6RrJS2RNE/S0IIFUq57Z6e22247pk2bxgUXXMCgQYMYPHgwTz/9NB9++CFnnXUWAwcOZMiQIZx77rl07dqVL3/5y9x9990+0GxmZVG0W2dLOgJ4B7g1IvZPx/0H8PeIuELShcAuEXGBpOOAc4DjgGHAzyJi2NaW4Vtnl47b1az1KMutsyPiCeDvtUafBNySvr8FODln/K2ReAboKqlXsWIzM7O6lfqYQs+IWJG+XwnU9MX0BpblTFedjjMzsxIq29lHERGSGt13JWkcMA6gb9++9dWNpOYFaJlt+el8rYH+tWnbckz65OfWlH+JLT76glTSMhSiXZv6NfOJJilIJYVT6j2FVTXdQunfN9Pxy4FP50zXJx23hYi4PiKqIqKq5p5CuTp27MiaNWv8RVYgEcGaNWvo2LFjuUMxsxIo9Z7CfcBo4Ir07705478jaSrJgeZ1Od1MjdKnTx+qq6tZvXp1IeI1kkTbp0+fvKb1ry+zbVvRkoKk24GjgB6SqoFJJMngTklnA68Dp6eTP0hy5tESYCPwzaYud/vtt6d///7NiNzMrO0qWlKIiFH1FI2oY9oAvl2sWMzMLD++otnMzDJOCmZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZxknBzMwyTgpmZpYp262zy823IzYz25L3FMzMLOOkYGZmGScFMzPLOCmYmVnGScHMzDJOCmZmltnqKamSqoDDgd2Ad4EFwCMR8XaRYzMzsxKrd09B0jclPQ9cBOwAvAS8CXweeFTSLZL6liZMMzMrhYb2FHYEDouId+sqlDQY2Av4WzECMzOz0qs3KUTEfzc0Y0TMLXw4ZmZWTnkfaJb0ZUkzJT0j6VvFDMrMzMqjoWMKg2uN+jowHDgUmFDMoMzMrDwaOqYwQdJ2wI8iYiWwDPgh8BHwRimCMzOz0mromMI/SxoE/ErSHOBS4BCSA9BXlyg+MzMroQaPKUTECxFxEvAX4F5gt4i4LyLeb85CJf2LpIWSFki6XVJHSf0lPStpiaQ7JLVvzjLMzKzxGjqmMF7S05KeBjoBxwBdJf1J0hFNXaCk3sC5QFVE7A9UACOBK4GfRsRngLeBs5u6DDMza5qG9hS+FRGHkhxcPj8iNkfEtSRf4Cc3c7ntgB0ktSPpjloBfAGYlpbfUoBlmJlZIzV0oHm5pItJvrQX14xMb2/x/aYuMCKWS7qa5KK3d4GHgTnA2ojYnE5WDfSua35J44BxAH37+oJqM7NCamhP4SRgPvAU8I1CLVDSLmnd/Unup1TTNZWXiLg+IqoioqqysrJQYZmZGQ3vKewWEdPrK5QkoHdEVDdymUcDr0XE6rSePwCHkRyvaJfuLfQBljeyXjMza6aG9hSuknSXpG9I2k/SpyT1lfQFST8G/g/YtwnL/BvwOUk7pollBPAi8BhwajrNaJKznczMrIQauk7hNEkDgDOBsUAvYCOwCHgQmBIR7zV2gRHxrKRpwPPAZpLTXa8HHgCmSro8HXdDY+s2M7PmafB5ChHxInBJoRcaEZOASbVGvwocXOhlmZlZ/vzkNTMzyzgpmJlZxknBzMwyW00KSpwl6dJ0uK8k9/2bmbVC+ewp/ILk7qij0uENQINPZTMzs21Tg2cfpYZFxFBJf4HkNhe+g6mZWeuUz57CJkkVQABIqiR50I6ZmbUy+SSFa4G7gU9JmkJyL6R/K2pUZmZWFlvtPoqI29Inr40ABJwcEYuKHpmZmZXcVpOCpG7Am8DtOeO2j4hNxQzMzMxKL5/uo+eB1cBfgZfT90slPS/pwGIGZ2ZmpZVPUngEOC4iekREd+BY4H7gWySnq5qZWSuRT1L4XET8qWYgIh4GDomIZ4AORYvMzMxKLp/rFFZIugCYmg5/DViVnqbqU1PNzFqRfPYUziB5Eto96atvOq4COL14oZmZWanlc0rqW8A59RQvKWw4ZmZWTvmckloJTAT2AzrWjI+ILxQxLjMzK4N8uo9uAxYD/YF/BZYCzxUxJjMzK5N8kkL3iLgB2BQRj0fEWMB7CWZmrVA+Zx/VXLm8QtLxwBtAt+KFZGZm5ZJPUrhc0s7AD4D/AnYCvlfUqMzMrCzySQpvR8Q6YB0wHEDSYUWNyszMyiKfYwr/lec4MzPbxtW7pyDpEOBQoFLS93OKdiK5cM3MzFqZhvYU2gOdSRJHl5zXeuDU5ixUUldJ0yQtlrRI0iGSukl6RNLL6d9dmrMMMzNrvHr3FCLiceBxSTdHxOsFXu7PgD9GxKnp8553BC4GZkTEFZIuBC4ELijwcs3MrAH5HGjuIOl6oF/u9E29ojk9k+kIYExazwfAB5JOAo5KJ7sFmImTgplZSeWTFH4PXAf8D/BhAZbZn+RBPTdJGgTMAb4L9IyIFek0K4Gedc0saRwwDqBv374FCMfMzGrkkxQ2R8QvC7zMocA5EfGspJ+RdBVlIiIkRV0zR8T1wPUAVVVVdU5jZmZNk88pqdMlfUtSr/RgcLf0uc1NVQ1UR8Sz6fA0kiSxSlIvgPTvm81YhpmZNUE+ewqj07/n54wLYI+mLDAiVkpaJmmfiHgJGAG8mL5GA1ekf+9tSv1mZtZ0+TxPoX8RlnsOcFt65tGrwDdJ9lrulHQ28Dp+gI+ZWcnl8zyFHYHvA30jYpykvYB9IuL+pi40IuYCVXUUjWhqnWZm1nz5HFO4CfiA5OpmgOXA5UWLyMzMyiafpLBnRPwH6S20I2IjoKJGZWZmZZFPUvhA0g4kB5eRtCfwflGjMjOzssjn7KNJwB+BT0u6DTiM9GpkMzNrXfI5++gRSc8DnyPpNvpuRLxV9MjMzKzkttp9JOkrJFc1P5CecbRZ0snFD83MzEotn2MKk9InrwEQEWtJupTMzKyVyScp1DVNPscizMxsG5NPUpgt6SeS9kxfPyG5s6mZmbUy+SSFc0guXrsDmAq8B3y7mEGZmVl5NNgNJKkCuD8ihpcoHjMzK6MG9xQi4kPgo/RpaWZm1srlc8D4HWC+pEeAf9SMjIhzixaVmZmVRT5J4Q/py8zMWrl8rmi+Jb33Ud/0oThmZtZK5XNF85eBuST3P0LSYEn3FTswMzMrvXxOSZ0MHAyshewBOU16FKeZmbVs+SSFTbm3uUh9VIxgzMysvPI50LxQ0hlARfooznOBp4sblpmZlUO+VzTvR/Jgnd8B64DvFTMoMzMrj3r3FCR1BMYDnwHmA4dExOZSBWZmZqXX0J7CLUAVSUI4Fri6JBGZmVnZNHRMYUBEDASQdAMwqzQhmZlZuTS0p7Cp5o27jczM2oaG9hQGSVqfvhewQzosICJip6JHZ2ZmJVVvUoiIimIuOL0t92xgeUScIKk/yfMaupM8xOfrEfFBMWMwM7NPyueU1GL5LrAoZ/hK4KcR8RngbeDsskRlZtaGlSUpSOoDHA/8Tzos4AvAtHSSW4CTyxGbmVlbVq49hWuAiXx8u4zuwNqcA9rVQO+6ZpQ0TtJsSbNXr15d/EjNzNqQkicFSScAb0bEnKbMHxHXR0RVRFRVVlYWODozs7Ytn3sfFdphwImSjgM6AjsBPwO6SmqX7i30AZaXITYzszat5HsKEXFRRPSJiH7ASOB/I+JM4DHg1HSy0cC9pY7NzKytK+fZR7VdAHxf0hKSYww3lDkeM7M2pxzdR5mImAnMTN+/SvIwHzMzK5OWtKdgZmZl5qRgZmYZJwUzM8s4KZiZWcZJwczMMk4KZmaWcVIwM7OMk4KZmWWcFMzMLOOkYGZmGScFMzPLOCmYmVnGScHMzDJOCmZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZxknBzMwyTgpmZpZxUjAzs4yTgpmZZUqeFCR9WtJjkl6UtFDSd9Px3SQ9Iunl9O8upY7NzKytK8eewmbgBxExAPgc8G1JA4ALgRkRsRcwIx02M7MSKnlSiIgVEfF8+n4DsAjoDZwE3JJOdgtwcqljMzNr68p6TEFSP2AI8CzQMyJWpEUrgZ71zDNO0mxJs1evXl2SOM3M2oqyJQVJnYG7gO9FxPrcsogIIOqaLyKuj4iqiKiqrKwsQaRmZm1HWZKCpO1JEsJtEfGHdPQqSb3S8l7Am+WIzcysLSvH2UcCbgAWRcRPcoruA0an70cD95Y6NjOztq5dGZZ5GPB1YL6kuem4i4ErgDslnQ28DpxehtjMzNq0kieFiHgKUD3FI0oZi5mZfZKvaDYzs4yTgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWcZJwczMMk4KZmaWcVIwM7OMk4KZmWWcFMzMLOOkYGZmGScFMzPLOCmYmVnGScHMzDJOCmZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZxknBzMwyTgpmZpZxUjAzs0yLSwqSjpH0kqQlki4sdzxmZm1Ji0oKkiqA/waOBQYAoyQNKG9UZmZtR4tKCsDBwJKIeDUiPgCmAieVOSYzszZDEVHuGDKSTgWOiYh/Soe/DgyLiO/kTDMOGJcO7gO8VPJA89MDeKvcQbRCbtfCc5sWR0tu190jorKugnaljqS5IuJ64Ppyx7E1kmZHRFW542ht3K6F5zYtjm21XVta99Fy4NM5w33ScWZmVgItLSk8B+wlqb+k9sBI4L4yx2Rm1ma0qO6jiNgs6TvAn4AK4MaIWFjmsJqqxXdxbaPcroXnNi2ObbJdW9SBZjMzK6+W1n1kZmZl5KRgZmYZJ4UikXSapIWSPpJUVavsovQ2Hi9J+lK5YtwW1deukvpJelfS3PR1XTnj3JZIukrSYknzJN0tqWtOmbfVJqqvXVv6tuqk0EyS2kvqVEfRAuAU4Ila0w8gOatqP+AY4Bfp7T0sR2PbNfVKRAxOX+OLG+G2p4E2fQTYPyIOAP4KXJRO7201D41t11SL3VadFJpI0r6S/pPkiuq9a5dHxKKIqOtq65OAqRHxfkS8Biwhub2H0ax2tXrk0aYPR8TmdPAZkuuDwNtqg5rRri2ak0IjSOok6ZuSngJ+DbwIHBARf2lENb2BZTnD1Y0m+g8AAAF0SURBVOm4NqtA7QrQX9JfJD0u6fDCR7rtaEabjgUeSt97W62lQO0KLXhbbVHXKWwDVgDzgH+KiMXlDqYVKUS7rgD6RsQaSQcC90jaLyLWFyzKbUuj21TSJcBm4LZiBraNK0S7tuht1XsKjXMqyW03/iDpUkm7N6EO38pjS81u17SLY036fg7wCnXs0rchjWpTSWOAE4Az4+OLl7ytbqnZ7drSt1UnhUZI+wi/BhwOrAPulfSopH6NqOY+YKSkDpL6A3sBswoe7DakEO0qqbLmIKikPUja9dUihLtNaEybSjoGmAicGBEbc4q8rdZSiHZt6duqr2huJkkHAysiYlmt8V8B/guoBNYCcyPiS2nZJSR9jJuB70XEQ9gnNLZdJX0VuAzYBHwETIqI6SUOu0VroE2XAB2ANemoZ2rOiPG2unWNbdeWvq06KZiZWcbdR2ZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZ5v8DkoudYIeFCNoAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# create dataset\n",
        "bars = list(sum(zip(perc_length_train, perc_length_valid, perc_length_test), ()))\n",
        "labels = ['< 10', '< 15', '< 20', '< 25']\n",
        "\n",
        "x_pos_bars = [1, 2, 3, 6, 7, 8, 11, 12, 13, 16, 17, 18]\n",
        "x_pos_labels = [2, 7, 12, 17]\n",
        "\n",
        "# Colors and mapping to values\n",
        "colors = ['g', 'b', 'r']\n",
        "colors_values = {'Train':'g', 'Validation':'b', 'Test': 'r'}    \n",
        "\n",
        "labels2 = list(colors_values.keys())\n",
        "handles = [plt.Rectangle((0,0),1,1, color=colors_values[label]) for label in labels2]\n",
        "# Make the plot\n",
        "plt.bar(x_pos_bars, bars,color = colors, label = colors_values)\n",
        "\n",
        "# Create names on the x-axis\n",
        "plt.xticks(x_pos_labels, labels)\n",
        "plt.legend(handles, labels2)\n",
        "plt.ylim([0, 130])\n",
        "#plt.xlabel('Metrics')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title('% of texts smaller than a given length')\n",
        "plt.show()"
      ],
      "id": "UoF5Nrlll08D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "655078ba"
      },
      "source": [
        "As we can see almost all news are smaller than **15** in length when tokenized so choosing this lenght to pad/truncate the tokenized news will not eliminate any information from the news in almost any case."
      ],
      "id": "655078ba"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "999ccdba"
      },
      "outputs": [],
      "source": [
        "# Pad/truncate the tokenized news\n",
        "\n",
        "# Train\n",
        "train_tokenized_pad = pad_sequences(train_tokenized, maxlen = 15, truncating = 'pre', padding = 'pre')\n",
        "# Validation\n",
        "valid_tokenized_pad = pad_sequences(valid_tokenized, maxlen = 15, truncating = 'pre', padding = 'pre')\n",
        "# Test\n",
        "test_tokenized_pad = pad_sequences(test_tokenized, maxlen = 15, truncating = 'pre', padding = 'pre')"
      ],
      "id": "999ccdba"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_e8qDaO1kf5"
      },
      "source": [
        "Now let's setup the GPU environment. The colab provides a free GPU to use. Do as follows:\n",
        "\n",
        "Runtime -> Change Runtime Type -> select GPU in Hardware accelerator\n",
        "Click connect on the top-right\n",
        "After connecting to one GPU, you can check its status using nvidia-smi comman"
      ],
      "id": "m_e8qDaO1kf5"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhKURGAb1f6_",
        "outputId": "ac0b365f-e1e7-48a0-cadf-2bd64e6651a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 18 03:16:53 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    30W /  70W |    840MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "torch.cuda.is_available()"
      ],
      "id": "nhKURGAb1f6_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c40705c"
      },
      "source": [
        "In order to use the data with torch we have to transform the arrays into dataloader objects but first they need to be transformed into tensors."
      ],
      "id": "3c40705c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781b4da2"
      },
      "source": [
        "### Word embeddings\n",
        "\n",
        "We will try different word embeddings and select the one which performs better. We create two functions: one to load the word embeddings and the other to create the embedding matrix."
      ],
      "id": "781b4da2"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "31ea6341"
      },
      "outputs": [],
      "source": [
        "# Function to load the word embeddings\n",
        "\n",
        "def load_embedd(filename):\n",
        "    words = []\n",
        "    vectors = []\n",
        "    file = open(filename,'r', encoding=\"utf8\")\n",
        "    for line in file.readlines():\n",
        "       row = line.split(' ')\n",
        "       vocab = row[0]\n",
        "       embd = row[1:len(row)]\n",
        "       embd[-1] = embd[-1].rstrip()\n",
        "       embd = list(map(float,embd)) # convert string to float\n",
        "       words.append(vocab)\n",
        "       vectors.append(embd)\n",
        "    file.close()\n",
        "    return words,vectors"
      ],
      "id": "31ea6341"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc4aef6f"
      },
      "source": [
        "Function to create the embedding matrix."
      ],
      "id": "bc4aef6f"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aa5f5128"
      },
      "outputs": [],
      "source": [
        "# Function to create the embedding matrix\n",
        "\n",
        "def embed_matx(word_index, vocab, embeddings, length_vocab, length_embedding):\n",
        "    embedding_matrix = np.zeros((length_vocab +1, length_embedding))\n",
        "    for word, i in word_index.items():\n",
        "        if word in vocab:\n",
        "            idx = vocab.index(word)\n",
        "            vector =  embeddings[idx]\n",
        "            embedding_matrix[i] = vector\n",
        "        if i == length_vocab:\n",
        "            break\n",
        "    return embedding_matrix"
      ],
      "id": "aa5f5128"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "991cf6d2"
      },
      "source": [
        "#### Glove (300 d)\n",
        "\n",
        "We use GloVe embeddings of dimension 300."
      ],
      "id": "991cf6d2"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dca6f8a7"
      },
      "outputs": [],
      "source": [
        "vocab_gv_300, vectors_gv_300 = load_embedd(filename = \"/content/drive/MyDrive/CS5344/Dataset/glove.6B.300d.txt\")"
      ],
      "id": "dca6f8a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b14586f"
      },
      "source": [
        "Now we create the embbeding matrix."
      ],
      "id": "8b14586f"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3c454f83"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer.word_index\n",
        "# Embedding matrix\n",
        "embedding_matrix_gv_300 = embed_matx(word_index = word_index, \n",
        "                                     vocab = vocab_gv_300, \n",
        "                                     embeddings = vectors_gv_300, \n",
        "                                     length_vocab = vocab_size, \n",
        "                                     length_embedding = 300)"
      ],
      "id": "3c454f83"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aadf419"
      },
      "source": [
        "## Model Training and Testing performance\n",
        "\n",
        "We join the **train** and **validation** datasets and we train the model with this dataset using the optimal number of epochs. Then we evaluate the model with the **test** set. First we need to modify a bit the class used previously."
      ],
      "id": "2aadf419"
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, nlabels, train_parameters = True, random_embeddings = False): \n",
        "        super().__init__()\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim = 300)\n",
        "        \n",
        "        if random_embeddings == True:\n",
        "            self.embedding.weight = nn.Parameter(torch.rand(vocab_size+1, 300), requires_grad = train_parameters)\n",
        "        else:\n",
        "            self.embedding.weight = nn.Parameter(torch.from_numpy(embedding_matrix_gv_300), requires_grad = train_parameters)\n",
        "            \n",
        "        # Filters for the CNN    \n",
        "        self.filter_sizes = [2,3,4,5]\n",
        "        self.num_filters = 50\n",
        "        \n",
        "        # Concolutional layers\n",
        "        self.convs_concat = nn.ModuleList([nn.Conv2d(1, self.num_filters, (K, 300)) for K in self.filter_sizes])\n",
        "        \n",
        "        # Linear layers\n",
        "        self.linear1 = nn.Linear(200,128)\n",
        "  \n",
        "        self.linear2 = nn.Linear(128,nlabels)\n",
        "    \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        x = self.embedding(x)\n",
        "        # Unsqueeze\n",
        "        x = x.unsqueeze(1)\n",
        "        # Convolution\n",
        "        x = [F.relu(conv(x.float())).squeeze(3) for conv in self.convs_concat]\n",
        "        # Max-pooling\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] \n",
        "        x = torch.cat(x, 1)\n",
        "        # Linear layers\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.logsoftmax(x) \n",
        "        return x"
      ],
      "metadata": {
        "id": "TI7tauNWGewL"
      },
      "id": "TI7tauNWGewL",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "c1bdb6d1"
      },
      "outputs": [],
      "source": [
        "class CNN_train_validation(CNN):\n",
        "    \n",
        "    def __init__(self,nlabels, train_parameters, random_embeddings, epochs=100,lr=0.001):\n",
        "        \n",
        "        super().__init__(nlabels, train_parameters, random_embeddings)  \n",
        "        self.lr = lr #Learning Rate\n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "        self.epochs = epochs\n",
        "        self.criterion = nn.NLLLoss()\n",
        "        # A list to store the loss evolution along training\n",
        "        self.loss_during_training = []\n",
        "        self.valid_loss_during_training = []\n",
        "        \n",
        "    def trainloop(self,trainloader):\n",
        "        \n",
        "        \n",
        "        # Optimization Loop\n",
        "        \n",
        "        for e in range(int(self.epochs)):\n",
        "\n",
        "            start_time = time.time()\n",
        "            # Random data permutation at each epoch\n",
        "            running_loss = 0.\n",
        "            i = 0\n",
        "            length = 0\n",
        "            accuracies = []\n",
        "            \n",
        "            for news, labels in trainloader:             \n",
        "        \n",
        "                self.optim.zero_grad()  # Reset gradients\n",
        "                out = self.forward(news.int())\n",
        "                loss = self.criterion(out,labels.long())\n",
        "                loss.backward()\n",
        "                running_loss += loss.item()\n",
        "                self.optim.step()\n",
        "                top_p, top_class = out.topk(1, dim=1)\n",
        "                equals = (top_class == labels.view(news.shape[0], 1))\n",
        "                length += news.shape[0]\n",
        "                accuracies.append(sum(equals))\n",
        "                accuracy = sum(accuracies)/length\n",
        "                i += 1\n",
        "                if i%1000 == 0:\n",
        "                    print(\" Train accuracy: \", accuracy)\n",
        "                \n",
        "            self.loss_during_training.append(running_loss/len(trainloader))\n",
        "            end_time = time.time()\n",
        "            print(\"Elapsed time for epoch: {e}: \", end_time-start_time)\n",
        "\n",
        "                \n",
        "    def eval_performance(self,dataloader):\n",
        "        predictions = np.empty((1,1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for news,labels in dataloader:\n",
        "                \n",
        "                logprobs = self.forward(news)  \n",
        "                top_p, top_class = logprobs.topk(1, dim=1)\n",
        "                \n",
        "                top_class_array = np.array(top_class)\n",
        "                predictions = np.concatenate((predictions, top_class_array), axis = 0)\n",
        "                \n",
        "        return predictions[1:]"
      ],
      "id": "c1bdb6d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "045b5f20"
      },
      "source": [
        "Now we join the train and validation sets."
      ],
      "id": "045b5f20"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7d243cda"
      },
      "outputs": [],
      "source": [
        "# Join train and validation sequences\n",
        "train_valid_tokenized_pad = np.concatenate((train_tokenized_pad, valid_tokenized_pad), axis = 0)\n",
        "# Join train and validation labels\n",
        "train_valid_labels = np.concatenate((np.array(train_labels), np.array(valid_labels)), axis = 0)\n",
        "\n",
        "# Create tensor objects\n",
        "\n",
        "# Train + validation\n",
        "train_valid_tensor = torch.Tensor(train_valid_tokenized_pad).int()\n",
        "# Test\n",
        "test_tensor =  torch.Tensor(test_tokenized_pad).int()\n",
        "\n",
        "# Tranform tensors into data loader objects\n",
        "\n",
        "# Train + validation\n",
        "train_valid_set = TensorDataset(train_valid_tensor, torch.Tensor(np.array(train_valid_labels)))\n",
        "train_valid_loader = DataLoader(train_valid_set, batch_size=64)\n",
        "# Test\n",
        "test_set = TensorDataset(test_tensor, torch.Tensor(np.array(test_labels)))\n",
        "testloader =  DataLoader(test_set, batch_size=64)"
      ],
      "id": "7d243cda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb3c5d4"
      },
      "source": [
        "### Random embeddings  + Training embeddings"
      ],
      "id": "4cb3c5d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0e7b4f4",
        "outputId": "f4a2952f-d3da-4e7d-d4ae-fc7ccbd15f76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5919])\n",
            " Train accuracy:  tensor([0.6198])\n",
            " Train accuracy:  tensor([0.6408])\n",
            " Train accuracy:  tensor([0.6534])\n",
            " Train accuracy:  tensor([0.6628])\n",
            " Train accuracy:  tensor([0.6697])\n",
            " Train accuracy:  tensor([0.6752])\n",
            " Train accuracy:  tensor([0.6800])\n",
            " Train accuracy:  tensor([0.6841])\n",
            " Train accuracy:  tensor([0.6874])\n",
            "Elapsed time:  7168.229678869247\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "CNN_test_train_random = CNN_extended(nlabels = 6, epochs=1, lr=0.003, train_parameters = True, random_embeddings = True)\n",
        "# Train model\n",
        "CNN_test_train_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions1 = CNN_test_train_random.eval_performance(testloader)"
      ],
      "id": "f0e7b4f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f64b0527",
        "outputId": "3a5199fc-6a2d-4581-fd01-21a776e84aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.87      0.79     23507\n",
            "           1       0.63      0.26      0.37      3514\n",
            "           2       0.70      0.48      0.57     11297\n",
            "           3       0.72      0.07      0.13      1224\n",
            "           4       0.75      0.84      0.79     17472\n",
            "           5       0.71      0.54      0.61      2305\n",
            "\n",
            "    accuracy                           0.72     59319\n",
            "   macro avg       0.70      0.51      0.54     59319\n",
            "weighted avg       0.72      0.72      0.70     59319\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions1))"
      ],
      "id": "f64b0527"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZGve0Wp5XMu",
        "outputId": "25926e5f-2712-4589-ae1e-69d5abc01400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.63      0.26      0.37      3514\n",
            "           2       0.70      0.48      0.57     11297\n",
            "           3       0.72      0.07      0.13      1224\n",
            "           4       0.75      0.84      0.79     17472\n",
            "           5       0.71      0.54      0.61      2305\n",
            "\n",
            "   micro avg       0.73      0.62      0.67     35812\n",
            "   macro avg       0.70      0.44      0.49     35812\n",
            "weighted avg       0.72      0.62      0.65     35812\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions1, labels = [1,2,3,4,5]))"
      ],
      "id": "UZGve0Wp5XMu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8c8aXFn5Wl1",
        "outputId": "bffd1e1f-6e75-4a7b-e7b6-1b41d909c33c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[20543   198  1057    12  1512   185]\n",
            " [ 1439   923   206     5   885    56]\n",
            " [ 3499   109  5391     7  2113   178]\n",
            " [  796    55    87    85   170    31]\n",
            " [ 1803   139   791     5 14672    62]\n",
            " [  706    51   131     4   172  1241]]\n"
          ]
        }
      ],
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions1))"
      ],
      "id": "g8c8aXFn5Wl1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe5ad30b"
      },
      "source": [
        "### GloVe embeddings  + Training embeddings"
      ],
      "id": "fe5ad30b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8797421",
        "outputId": "0e3ad4b6-0e28-4b7a-9357-01fb7e89f283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.7781])\n",
            " Train accuracy:  tensor([0.7868])\n",
            " Train accuracy:  tensor([0.7934])\n",
            " Train accuracy:  tensor([0.7974])\n",
            " Train accuracy:  tensor([0.7999])\n",
            " Train accuracy:  tensor([0.8032])\n",
            " Train accuracy:  tensor([0.8052])\n",
            " Train accuracy:  tensor([0.8069])\n",
            " Train accuracy:  tensor([0.8083])\n",
            " Train accuracy:  tensor([0.8097])\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "CNN_test_train_not_random = CNN_train_validation(\n",
        "    nlabels = 2, epochs=5, lr=0.003, train_parameters = True, random_embeddings = False)\n",
        "# Train model\n",
        "CNN_test_train_not_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions_2 = CNN_test_train_not_random.eval_performance(testloader)"
      ],
      "id": "c8797421"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77e3853a"
      },
      "outputs": [],
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_2))"
      ],
      "id": "77e3853a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6xDZyYx5xEz"
      },
      "outputs": [],
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_2, labels = [1,2,3,4,5]))"
      ],
      "id": "x6xDZyYx5xEz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umI_sor55xkl"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_2))"
      ],
      "id": "umI_sor55xkl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ae61424"
      },
      "source": [
        "### Random embeddings  +  Not training embeddings"
      ],
      "id": "6ae61424"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e25f549b",
        "outputId": "7fa6bb9d-5b84-41a8-9f48-5806a7981cd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5530])\n",
            " Train accuracy:  tensor([0.5597])\n",
            " Train accuracy:  tensor([0.5638])\n",
            " Train accuracy:  tensor([0.5688])\n",
            " Train accuracy:  tensor([0.5730])\n",
            " Train accuracy:  tensor([0.5763])\n",
            " Train accuracy:  tensor([0.5796])\n",
            " Train accuracy:  tensor([0.5827])\n",
            " Train accuracy:  tensor([0.5856])\n",
            " Train accuracy:  tensor([0.5880])\n",
            "Elapsed time:  743.8465197086334\n",
            " Train accuracy:  tensor([0.6123])\n",
            " Train accuracy:  tensor([0.6118])\n",
            " Train accuracy:  tensor([0.6123])\n",
            " Train accuracy:  tensor([0.6127])\n",
            " Train accuracy:  tensor([0.6134])\n",
            " Train accuracy:  tensor([0.6141])\n",
            " Train accuracy:  tensor([0.6147])\n",
            " Train accuracy:  tensor([0.6150])\n",
            " Train accuracy:  tensor([0.6156])\n",
            " Train accuracy:  tensor([0.6159])\n",
            "Elapsed time:  741.4050281047821\n",
            " Train accuracy:  tensor([0.6202])\n",
            " Train accuracy:  tensor([0.6200])\n",
            " Train accuracy:  tensor([0.6203])\n",
            " Train accuracy:  tensor([0.6208])\n",
            " Train accuracy:  tensor([0.6211])\n",
            " Train accuracy:  tensor([0.6217])\n",
            " Train accuracy:  tensor([0.6220])\n",
            " Train accuracy:  tensor([0.6223])\n",
            " Train accuracy:  tensor([0.6228])\n",
            " Train accuracy:  tensor([0.6231])\n",
            "Elapsed time:  735.0587935447693\n",
            " Train accuracy:  tensor([0.6248])\n",
            " Train accuracy:  tensor([0.6242])\n",
            " Train accuracy:  tensor([0.6247])\n",
            " Train accuracy:  tensor([0.6246])\n",
            " Train accuracy:  tensor([0.6249])\n",
            " Train accuracy:  tensor([0.6252])\n",
            " Train accuracy:  tensor([0.6253])\n",
            " Train accuracy:  tensor([0.6254])\n",
            " Train accuracy:  tensor([0.6259])\n",
            " Train accuracy:  tensor([0.6261])\n",
            "Elapsed time:  742.421392917633\n",
            " Train accuracy:  tensor([0.6267])\n",
            " Train accuracy:  tensor([0.6266])\n",
            " Train accuracy:  tensor([0.6264])\n",
            " Train accuracy:  tensor([0.6265])\n",
            " Train accuracy:  tensor([0.6271])\n",
            " Train accuracy:  tensor([0.6275])\n",
            " Train accuracy:  tensor([0.6277])\n",
            " Train accuracy:  tensor([0.6279])\n",
            " Train accuracy:  tensor([0.6283])\n",
            " Train accuracy:  tensor([0.6284])\n",
            "Elapsed time:  757.4541792869568\n",
            " Train accuracy:  tensor([0.6299])\n",
            " Train accuracy:  tensor([0.6288])\n",
            " Train accuracy:  tensor([0.6288])\n",
            " Train accuracy:  tensor([0.6286])\n",
            " Train accuracy:  tensor([0.6289])\n",
            " Train accuracy:  tensor([0.6291])\n",
            " Train accuracy:  tensor([0.6292])\n",
            " Train accuracy:  tensor([0.6292])\n",
            " Train accuracy:  tensor([0.6295])\n",
            " Train accuracy:  tensor([0.6296])\n",
            "Elapsed time:  782.0601441860199\n",
            " Train accuracy:  tensor([0.6310])\n",
            " Train accuracy:  tensor([0.6302])\n",
            " Train accuracy:  tensor([0.6298])\n",
            " Train accuracy:  tensor([0.6298])\n",
            " Train accuracy:  tensor([0.6299])\n",
            " Train accuracy:  tensor([0.6303])\n",
            " Train accuracy:  tensor([0.6303])\n",
            " Train accuracy:  tensor([0.6302])\n",
            " Train accuracy:  tensor([0.6306])\n",
            " Train accuracy:  tensor([0.6306])\n",
            "Elapsed time:  836.1870765686035\n",
            " Train accuracy:  tensor([0.6324])\n",
            " Train accuracy:  tensor([0.6313])\n",
            " Train accuracy:  tensor([0.6309])\n",
            " Train accuracy:  tensor([0.6307])\n",
            " Train accuracy:  tensor([0.6308])\n",
            " Train accuracy:  tensor([0.6311])\n",
            " Train accuracy:  tensor([0.6311])\n",
            " Train accuracy:  tensor([0.6309])\n",
            " Train accuracy:  tensor([0.6312])\n",
            " Train accuracy:  tensor([0.6311])\n",
            "Elapsed time:  902.3521666526794\n",
            " Train accuracy:  tensor([0.6333])\n",
            " Train accuracy:  tensor([0.6323])\n",
            " Train accuracy:  tensor([0.6316])\n",
            " Train accuracy:  tensor([0.6315])\n",
            " Train accuracy:  tensor([0.6314])\n",
            " Train accuracy:  tensor([0.6317])\n",
            " Train accuracy:  tensor([0.6316])\n",
            " Train accuracy:  tensor([0.6315])\n",
            " Train accuracy:  tensor([0.6318])\n",
            " Train accuracy:  tensor([0.6318])\n",
            "Elapsed time:  902.0216271877289\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "CNN_test_not_train_random = CNN_extended(nlabels = 6, epochs=9, lr=0.003, train_parameters = False, random_embeddings = True)\n",
        "# Train model\n",
        "CNN_test_not_train_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions_3 = CNN_test_not_train_random.eval_performance(testloader)"
      ],
      "id": "e25f549b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f068b98",
        "outputId": "cbeb5654-2284-4081-ee94-eb126935d175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.81      0.71     23507\n",
            "           1       0.81      0.03      0.06      3514\n",
            "           2       0.52      0.34      0.41     11297\n",
            "           3       0.30      0.02      0.04      1224\n",
            "           4       0.67      0.78      0.72     17472\n",
            "           5       0.69      0.29      0.41      2305\n",
            "\n",
            "    accuracy                           0.63     59319\n",
            "   macro avg       0.60      0.38      0.39     59319\n",
            "weighted avg       0.63      0.63      0.59     59319\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_3))"
      ],
      "id": "8f068b98"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H34ptx3whpOS",
        "outputId": "4b0541c3-4649-4036-9da9-8038119847cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.81      0.03      0.06      3514\n",
            "           2       0.52      0.34      0.41     11297\n",
            "           3       0.30      0.02      0.04      1224\n",
            "           4       0.67      0.78      0.72     17472\n",
            "           5       0.69      0.29      0.41      2305\n",
            "\n",
            "   micro avg       0.63      0.51      0.56     35812\n",
            "   macro avg       0.60      0.29      0.33     35812\n",
            "weighted avg       0.62      0.51      0.51     35812\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_3, labels = [1,2,3,4,5]))"
      ],
      "id": "H34ptx3whpOS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1koxLr3NhpDw",
        "outputId": "baa0ed16-8c0c-42d3-fdec-787cf74cc78e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[19038    13  1838    22  2467   129]\n",
            " [ 2119   112   288     8   953    34]\n",
            " [ 4362     5  3845     3  2992    90]\n",
            " [  904     0   123    29   150    18]\n",
            " [ 2792     7   974    24 13640    35]\n",
            " [ 1093     2   307    11   221   671]]\n"
          ]
        }
      ],
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_3))"
      ],
      "id": "1koxLr3NhpDw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43a272b"
      },
      "source": [
        "### GloVe embeddings  +  Not training embeddings"
      ],
      "id": "e43a272b"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "430ceef0",
        "outputId": "424b2805-d8da-4438-e79e-0eca949838d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Train accuracy:  tensor([0.7625])\n",
            " Train accuracy:  tensor([0.7698])\n",
            " Train accuracy:  tensor([0.7762])\n",
            " Train accuracy:  tensor([0.7802])\n",
            " Train accuracy:  tensor([0.7825])\n",
            " Train accuracy:  tensor([0.7856])\n",
            " Train accuracy:  tensor([0.7877])\n",
            " Train accuracy:  tensor([0.7894])\n",
            " Train accuracy:  tensor([0.7908])\n",
            " Train accuracy:  tensor([0.7920])\n",
            " Train accuracy:  tensor([0.7931])\n",
            " Train accuracy:  tensor([0.7943])\n",
            " Train accuracy:  tensor([0.7952])\n",
            "Elapsed time:  717.7152919769287\n",
            " Train accuracy:  tensor([0.8108])\n",
            " Train accuracy:  tensor([0.8104])\n",
            " Train accuracy:  tensor([0.8117])\n",
            " Train accuracy:  tensor([0.8125])\n",
            " Train accuracy:  tensor([0.8125])\n",
            " Train accuracy:  tensor([0.8140])\n",
            " Train accuracy:  tensor([0.8148])\n",
            " Train accuracy:  tensor([0.8155])\n",
            " Train accuracy:  tensor([0.8161])\n",
            " Train accuracy:  tensor([0.8166])\n",
            " Train accuracy:  tensor([0.8172])\n",
            " Train accuracy:  tensor([0.8176])\n",
            " Train accuracy:  tensor([0.8179])\n",
            "Elapsed time:  710.1242599487305\n",
            " Train accuracy:  tensor([0.8265])\n",
            " Train accuracy:  tensor([0.8263])\n",
            " Train accuracy:  tensor([0.8275])\n",
            " Train accuracy:  tensor([0.8275])\n",
            " Train accuracy:  tensor([0.8274])\n",
            " Train accuracy:  tensor([0.8286])\n",
            " Train accuracy:  tensor([0.8289])\n",
            " Train accuracy:  tensor([0.8296])\n",
            " Train accuracy:  tensor([0.8300])\n",
            " Train accuracy:  tensor([0.8303])\n",
            " Train accuracy:  tensor([0.8306])\n",
            " Train accuracy:  tensor([0.8309])\n",
            " Train accuracy:  tensor([0.8311])\n",
            "Elapsed time:  707.2323369979858\n",
            " Train accuracy:  tensor([0.8373])\n",
            " Train accuracy:  tensor([0.8365])\n",
            " Train accuracy:  tensor([0.8380])\n",
            " Train accuracy:  tensor([0.8378])\n",
            " Train accuracy:  tensor([0.8377])\n",
            " Train accuracy:  tensor([0.8388])\n",
            " Train accuracy:  tensor([0.8389])\n",
            " Train accuracy:  tensor([0.8394])\n",
            " Train accuracy:  tensor([0.8397])\n",
            " Train accuracy:  tensor([0.8400])\n",
            " Train accuracy:  tensor([0.8401])\n",
            " Train accuracy:  tensor([0.8404])\n",
            " Train accuracy:  tensor([0.8404])\n",
            "Elapsed time:  717.9036550521851\n",
            " Train accuracy:  tensor([0.8445])\n",
            " Train accuracy:  tensor([0.8438])\n",
            " Train accuracy:  tensor([0.8450])\n",
            " Train accuracy:  tensor([0.8450])\n",
            " Train accuracy:  tensor([0.8449])\n",
            " Train accuracy:  tensor([0.8458])\n",
            " Train accuracy:  tensor([0.8458])\n",
            " Train accuracy:  tensor([0.8462])\n",
            " Train accuracy:  tensor([0.8466])\n",
            " Train accuracy:  tensor([0.8469])\n",
            " Train accuracy:  tensor([0.8471])\n",
            " Train accuracy:  tensor([0.8472])\n",
            " Train accuracy:  tensor([0.8473])\n",
            "Elapsed time:  723.3734700679779\n",
            " Train accuracy:  tensor([0.8505])\n",
            " Train accuracy:  tensor([0.8493])\n",
            " Train accuracy:  tensor([0.8505])\n",
            " Train accuracy:  tensor([0.8502])\n",
            " Train accuracy:  tensor([0.8503])\n",
            " Train accuracy:  tensor([0.8514])\n",
            " Train accuracy:  tensor([0.8516])\n",
            " Train accuracy:  tensor([0.8520])\n",
            " Train accuracy:  tensor([0.8522])\n",
            " Train accuracy:  tensor([0.8523])\n",
            " Train accuracy:  tensor([0.8523])\n",
            " Train accuracy:  tensor([0.8526])\n",
            " Train accuracy:  tensor([0.8526])\n",
            "Elapsed time:  727.936918258667\n",
            " Train accuracy:  tensor([0.8548])\n",
            " Train accuracy:  tensor([0.8544])\n",
            " Train accuracy:  tensor([0.8556])\n",
            " Train accuracy:  tensor([0.8556])\n",
            " Train accuracy:  tensor([0.8557])\n",
            " Train accuracy:  tensor([0.8565])\n",
            " Train accuracy:  tensor([0.8566])\n",
            " Train accuracy:  tensor([0.8570])\n",
            " Train accuracy:  tensor([0.8570])\n",
            " Train accuracy:  tensor([0.8572])\n",
            " Train accuracy:  tensor([0.8571])\n",
            " Train accuracy:  tensor([0.8573])\n",
            " Train accuracy:  tensor([0.8573])\n",
            "Elapsed time:  737.3790860176086\n",
            " Train accuracy:  tensor([0.8603])\n",
            " Train accuracy:  tensor([0.8594])\n",
            " Train accuracy:  tensor([0.8606])\n",
            " Train accuracy:  tensor([0.8602])\n",
            " Train accuracy:  tensor([0.8599])\n",
            " Train accuracy:  tensor([0.8607])\n",
            " Train accuracy:  tensor([0.8606])\n",
            " Train accuracy:  tensor([0.8610])\n",
            " Train accuracy:  tensor([0.8610])\n",
            " Train accuracy:  tensor([0.8610])\n",
            " Train accuracy:  tensor([0.8610])\n",
            " Train accuracy:  tensor([0.8611])\n",
            " Train accuracy:  tensor([0.8611])\n",
            "Elapsed time:  761.701653957367\n",
            " Train accuracy:  tensor([0.8643])\n",
            " Train accuracy:  tensor([0.8633])\n",
            " Train accuracy:  tensor([0.8640])\n",
            " Train accuracy:  tensor([0.8641])\n",
            " Train accuracy:  tensor([0.8641])\n",
            " Train accuracy:  tensor([0.8647])\n",
            " Train accuracy:  tensor([0.8647])\n",
            " Train accuracy:  tensor([0.8650])\n",
            " Train accuracy:  tensor([0.8647])\n",
            " Train accuracy:  tensor([0.8648])\n",
            " Train accuracy:  tensor([0.8646])\n",
            " Train accuracy:  tensor([0.8647])\n",
            " Train accuracy:  tensor([0.8646])\n",
            "Elapsed time:  736.3719792366028\n",
            " Train accuracy:  tensor([0.8674])\n",
            " Train accuracy:  tensor([0.8667])\n",
            " Train accuracy:  tensor([0.8670])\n",
            " Train accuracy:  tensor([0.8665])\n",
            " Train accuracy:  tensor([0.8666])\n",
            " Train accuracy:  tensor([0.8673])\n",
            " Train accuracy:  tensor([0.8672])\n",
            " Train accuracy:  tensor([0.8675])\n",
            " Train accuracy:  tensor([0.8673])\n",
            " Train accuracy:  tensor([0.8674])\n",
            " Train accuracy:  tensor([0.8672])\n",
            " Train accuracy:  tensor([0.8674])\n",
            " Train accuracy:  tensor([0.8673])\n",
            "Elapsed time:  716.9079484939575\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "CNN_test_not_train_not_random = CNN_train_validation(\n",
        "    nlabels = 2, epochs=10, lr=0.003, train_parameters = False, random_embeddings = False)\n",
        "# Train model\n",
        "CNN_test_not_train_not_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions_4 = CNN_test_not_train_not_random.eval_performance(testloader)"
      ],
      "id": "430ceef0"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4b898225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0888418-d418-4d99-fd34-38230428a490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.83      0.81     42155\n",
            "           1       0.82      0.77      0.80     42326\n",
            "\n",
            "    accuracy                           0.80     84481\n",
            "   macro avg       0.80      0.80      0.80     84481\n",
            "weighted avg       0.80      0.80      0.80     84481\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_4))"
      ],
      "id": "4b898225"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "c50Y1ObIh35B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291a8714-38f2-47ce-bd69-2404c9f16a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.82      0.77      0.80     42326\n",
            "           2       0.00      0.00      0.00         0\n",
            "           3       0.00      0.00      0.00         0\n",
            "           4       0.00      0.00      0.00         0\n",
            "           5       0.00      0.00      0.00         0\n",
            "\n",
            "   micro avg       0.82      0.77      0.80     42326\n",
            "   macro avg       0.16      0.15      0.16     42326\n",
            "weighted avg       0.82      0.77      0.80     42326\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_4, labels = [1,2,3,4,5]))"
      ],
      "id": "c50Y1ObIh35B"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mtagSNrTh3yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1991dc0b-21f5-43bc-de40-0cc0da785089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[34995  7160]\n",
            " [ 9603 32723]]\n"
          ]
        }
      ],
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_4))"
      ],
      "id": "mtagSNrTh3yi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3yh-XIegD3a"
      },
      "outputs": [],
      "source": [],
      "id": "_3yh-XIegD3a"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4cb3c5d4",
        "6ae61424"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}